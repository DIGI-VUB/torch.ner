{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch-bilstm-rstats.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "649KiOAPMBH6",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "Install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT0lImSrGUAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(!\"word2vec\"      %in% rownames(installed.packages())) install.packages(\"word2vec\")\n",
        "if(!\"bit\"           %in% rownames(installed.packages())) install.packages(\"bit\")\n",
        "if(!\"crfsuite\"      %in% rownames(installed.packages())) install.packages(\"crfsuite\")\n",
        "if(!\"remotes\"       %in% rownames(installed.packages())) install.packages(\"remotes\")\n",
        "if(!\"sentencepiece\" %in% rownames(installed.packages())) remotes::install_github(\"bnosac/sentencepiece\")\n",
        "if(!\"torch\"         %in% rownames(installed.packages())) remotes::install_github(\"mlverse/torch\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy0DQjfyHdkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "suppressPackageStartupMessages({\n",
        "  library(sentencepiece) \n",
        "  library(word2vec)\n",
        "  library(bit)\n",
        "  library(crfsuite)\n",
        "})\n",
        "library(torch)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoSvtTCLNL5B",
        "colab_type": "text"
      },
      "source": [
        "## Download pretrained word2vec embeddings on subwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCvaLJHxNSZV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9b41b373-7d38-4b8e-f0f5-dc1ecb812607"
      },
      "source": [
        "downloads <- sentencepiece_download_model(\"dutch\", vocab_size = 25000, dim = 25, model_dir = getwd())\n",
        "bpemb <- BPEembed(file_sentencepiece = downloads$file_model,\n",
        "                  file_word2vec = downloads$glove.bin$file_model)\n",
        "bpemb                  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "A sentencepiece encoder with 25000 subwords and embedding dimension 25\n",
              "  - Based on model file /content/nl.wiki.bpe.vs25000.model\n",
              "  - Based on wordvectors in /content/data/nl/nl.wiki.bpe.vs25000.d25.w2v.bin"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0DPXjCfNu3Y",
        "colab_type": "text"
      },
      "source": [
        "## Get training data for doing named entity recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAK-48HNNwng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "9ba8b8e8-5c39-4135-f3ad-1e30b1964349"
      },
      "source": [
        "##\n",
        "## Get some training data\n",
        "##\n",
        "conll2002 <- ner_download_modeldata(\"conll2002-nl\")\n",
        "conll2002$label <- factor(conll2002$label, levels = c(\"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\", \"B-ORG\", \"I-ORG\", \"B-PER\", \"I-PER\", \"O\"))\n",
        "conll2002$label_nr <- as.integer(conll2002$label) - 1L\n",
        "head(conll2002)\n",
        "table(conll2002$data)\n",
        "table(conll2002$label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  data      doc_id sentence_id token  pos  label label_nr\n",
              "1 ned.train 1      1           De     Art  O     8       \n",
              "2 ned.train 1      1           tekst  N    O     8       \n",
              "3 ned.train 1      1           van    Prep O     8       \n",
              "4 ned.train 1      1           het    Art  O     8       \n",
              "5 ned.train 1      1           arrest N    O     8       \n",
              "6 ned.train 1      1           is     V    O     8       "
            ],
            "text/latex": "A data.table: 6 × 7\n\\begin{tabular}{lllllll}\n data & doc\\_id & sentence\\_id & token & pos & label & label\\_nr\\\\\n <chr> & <int> & <int> & <chr> & <chr> & <fct> & <int>\\\\\n\\hline\n\t ned.train & 1 & 1 & De     & Art  & O & 8\\\\\n\t ned.train & 1 & 1 & tekst  & N    & O & 8\\\\\n\t ned.train & 1 & 1 & van    & Prep & O & 8\\\\\n\t ned.train & 1 & 1 & het    & Art  & O & 8\\\\\n\t ned.train & 1 & 1 & arrest & N    & O & 8\\\\\n\t ned.train & 1 & 1 & is     & V    & O & 8\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.table: 6 × 7\n\n| data &lt;chr&gt; | doc_id &lt;int&gt; | sentence_id &lt;int&gt; | token &lt;chr&gt; | pos &lt;chr&gt; | label &lt;fct&gt; | label_nr &lt;int&gt; |\n|---|---|---|---|---|---|---|\n| ned.train | 1 | 1 | De     | Art  | O | 8 |\n| ned.train | 1 | 1 | tekst  | N    | O | 8 |\n| ned.train | 1 | 1 | van    | Prep | O | 8 |\n| ned.train | 1 | 1 | het    | Art  | O | 8 |\n| ned.train | 1 | 1 | arrest | N    | O | 8 |\n| ned.train | 1 | 1 | is     | V    | O | 8 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.table: 6 × 7</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>data</th><th scope=col>doc_id</th><th scope=col>sentence_id</th><th scope=col>token</th><th scope=col>pos</th><th scope=col>label</th><th scope=col>label_nr</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>De    </td><td>Art </td><td>O</td><td>8</td></tr>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>tekst </td><td>N   </td><td>O</td><td>8</td></tr>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>van   </td><td>Prep</td><td>O</td><td>8</td></tr>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>het   </td><td>Art </td><td>O</td><td>8</td></tr>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>arrest</td><td>N   </td><td>O</td><td>8</td></tr>\n",
              "\t<tr><td>ned.train</td><td>1</td><td>1</td><td>is    </td><td>V   </td><td>O</td><td>8</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "ned.train     testa     testb \n",
              "   202644     37687     68875 "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              " B-LOC  I-LOC B-MISC I-MISC  B-ORG  I-ORG  B-PER  I-PER      O \n",
              "  4461    580   5273   2030   3650   2146   6517   4113 280436 "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RsGllEgOUi9",
        "colab_type": "text"
      },
      "source": [
        "## Split in train / test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg5xkKuuOYZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "b3a73fb6-1f33-4946-fcf5-ea59549bbf66"
      },
      "source": [
        "traindata <- subset(conll2002, data %in% \"ned.train\")\n",
        "traindata <- split(traindata, traindata$doc_id)\n",
        "testdata  <- subset(conll2002, data %in% c(\"testa\", \"testb\"))\n",
        "testdata  <- split(testdata, testdata$doc_id)\n",
        "str(traindata[1:3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of 3\n",
            " $ 1:Classes ‘data.table’ and 'data.frame':\t749 obs. of  7 variables:\n",
            "  ..$ data       : chr [1:749] \"ned.train\" \"ned.train\" \"ned.train\" \"ned.train\" ...\n",
            "  ..$ doc_id     : int [1:749] 1 1 1 1 1 1 1 1 1 1 ...\n",
            "  ..$ sentence_id: int [1:749] 1 1 1 1 1 1 1 1 1 1 ...\n",
            "  ..$ token      : chr [1:749] \"De\" \"tekst\" \"van\" \"het\" ...\n",
            "  ..$ pos        : chr [1:749] \"Art\" \"N\" \"Prep\" \"Art\" ...\n",
            "  ..$ label      : Factor w/ 9 levels \"B-LOC\",\"I-LOC\",..: 9 9 9 9 9 9 9 9 9 9 ...\n",
            "  ..$ label_nr   : int [1:749] 8 8 8 8 8 8 8 8 8 8 ...\n",
            "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
            " $ 2:Classes ‘data.table’ and 'data.frame':\t713 obs. of  7 variables:\n",
            "  ..$ data       : chr [1:713] \"ned.train\" \"ned.train\" \"ned.train\" \"ned.train\" ...\n",
            "  ..$ doc_id     : int [1:713] 2 2 2 2 2 2 2 2 2 2 ...\n",
            "  ..$ sentence_id: int [1:713] 369 369 369 369 369 369 369 369 369 369 ...\n",
            "  ..$ token      : chr [1:713] \"Voor\" \"het\" \"eerst\" \"in\" ...\n",
            "  ..$ pos        : chr [1:713] \"Prep\" \"Art\" \"Adj\" \"Prep\" ...\n",
            "  ..$ label      : Factor w/ 9 levels \"B-LOC\",\"I-LOC\",..: 9 9 9 9 9 3 9 9 9 9 ...\n",
            "  ..$ label_nr   : int [1:713] 8 8 8 8 8 2 8 8 8 8 ...\n",
            "  ..- attr(*, \".internal.selfref\")=<externalptr> \n",
            " $ 3:Classes ‘data.table’ and 'data.frame':\t485 obs. of  7 variables:\n",
            "  ..$ data       : chr [1:485] \"ned.train\" \"ned.train\" \"ned.train\" \"ned.train\" ...\n",
            "  ..$ doc_id     : int [1:485] 3 3 3 3 3 3 3 3 3 3 ...\n",
            "  ..$ sentence_id: int [1:485] 4414 4414 4414 4414 4414 4414 4414 4414 4414 4414 ...\n",
            "  ..$ token      : chr [1:485] \"Eritrea\" \"wil\" \"terugkeren\" \"naar\" ...\n",
            "  ..$ pos        : chr [1:485] \"N\" \"V\" \"V\" \"Prep\" ...\n",
            "  ..$ label      : Factor w/ 9 levels \"B-LOC\",\"I-LOC\",..: 1 9 9 9 9 9 9 9 9 9 ...\n",
            "  ..$ label_nr   : int [1:485] 0 8 8 8 8 8 8 8 8 8 ...\n",
            "  ..- attr(*, \".internal.selfref\")=<externalptr> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onwtWl41NzPk",
        "colab_type": "text"
      },
      "source": [
        "## Define the model and the optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw3wNpSSN3bP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##\n",
        "## Define the BiLSTM *model*\n",
        "##\n",
        "##\n",
        "## DOCS AT https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "##\n",
        "ner_bilstm <- nn_module(\n",
        "  classname = \"ner_bilstm\",\n",
        "  initialize = function(bpemb, tagset_size, hidden_size = 32, num_layers = 1, nonlinearity = c(\"tanh\", \"relu\"), dropout = 0) {\n",
        "    nonlinearity <- match.arg(nonlinearity)\n",
        "    self$bpemb          <- bpemb\n",
        "    self$embedding_dim  <- bpemb$dim\n",
        "    self$tagset_size    <- as.integer(tagset_size)\n",
        "    self$num_layers     <- as.integer(num_layers)\n",
        "    self$hidden_size    <- as.integer(hidden_size)\n",
        "    \n",
        "    ## LSTM Layer\n",
        "    self$rnn       <- nn_rnn(mode = \"LSTM\", input_size = self$embedding_dim, \n",
        "                             hidden_size = self$hidden_size, num_layers = self$num_layers,\n",
        "                             dropout = dropout, bidirectional = TRUE, nonlinearity = nonlinearity, batch_first = TRUE)\n",
        "    ## Map of LSTM features to tag space\n",
        "    self$linear    <- nn_linear(in_features = hidden_size * 2, out_features = tagset_size, bias = TRUE)\n",
        "  },\n",
        "  forward = function(x, enforce_sorted = FALSE) {\n",
        "    x <- x[order(sapply(x, length), decreasing = TRUE)]\n",
        "    ## Tokenize and get the embedding of the tokens - sentencepiece splits according to subwords, take the average embedding of the subwords\n",
        "    ## Result is a list of matrices with as rows the words and as columns the embeddings\n",
        "    emb <- lapply(x, FUN=function(tokensequence){\n",
        "      emb <- predict(self$bpemb, newdata = tokensequence, type = \"encode\")\n",
        "      emb <- lapply(emb, colMeans)\n",
        "      emb <- do.call(rbind, emb)  \n",
        "      emb <- torch_tensor(emb, dtype = torch_float())\n",
        "      emb\n",
        "    })\n",
        "    ## Order by number of embedded words as that is needed by nn_utils_rnn_pack_sequence\n",
        "    p <- nn_utils_rnn_pack_sequence(emb, enforce_sorted = FALSE)\n",
        "    ## Forward pass, getting the LSTM features\n",
        "    rnn_out <- self$rnn(p)\n",
        "    rnn_out <- rnn_out[[1]]\n",
        "    ## Put the LSTM feature in 1 line per token and do a Forward pass over the linear layer \n",
        "    tag_space  <- self$linear(rnn_out$data)\n",
        "    ## Softmax transformation\n",
        "    tag_scores <- nnf_log_softmax(tag_space, dim = 1L)\n",
        "    tag_scores\n",
        "  }\n",
        ")\n",
        "##\n",
        "## Setup model (number of entities is 9 location / misc / organisation / person + other)\n",
        "##\n",
        "model <- ner_bilstm(bpemb = bpemb, tagset_size = 9, hidden_size = 4, num_layers = 1)\n",
        "optimizer <- optim_sgd(model$parameters, lr = 0.1, momentum = 0.9, weight_decay = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_3nqU9_OBX6",
        "colab_type": "text"
      },
      "source": [
        "## Train the model and evaluate on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HIwAqmfOElQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "74b4be44-f139-4212-ab68-4ad1449e3888"
      },
      "source": [
        "##\n",
        "## Loop over the training data, update the parameters\n",
        "##\n",
        "overview <- list()\n",
        "for(epoch in 1:10){\n",
        "  overview[[epoch]] <- list()\n",
        "  overview[[epoch]]$epoch <- epoch\n",
        "  cat(sprintf(\"%s epoch %s\", Sys.time(), epoch), sep = \"\\n\")\n",
        "  ###############################################################################\n",
        "  ## TRAIN\n",
        "  ##   - in batches of 50\n",
        "  ##   - using negative log loss\n",
        "  ###############################################################################\n",
        "  model$train()\n",
        "  ## shuffle training data\n",
        "  traindata <- sample(traindata)\n",
        "  chunks <- chunk(from = 1L, to = length(traindata), by = 50L, maxindex = length(traindata))\n",
        "  for(rangeindex in chunks){\n",
        "    ## Get a batch of sentences the sentences (list of words)\n",
        "    train_sentences <- as.which(rangeindex)\n",
        "    train_sentences <- traindata[train_sentences]\n",
        "    train_tokens    <- lapply(train_sentences, FUN = function(x) x$token)\n",
        "    ## Get the known value of the entity\n",
        "    train_target <- lapply(train_sentences, FUN = function(x) x$label_nr)\n",
        "    train_target <- unlist(train_target)\n",
        "    train_target <- torch_tensor(train_target, dtype = torch_long())\n",
        "    \n",
        "    ## Forward pass, calculate loss, do backward pass\n",
        "    optimizer$zero_grad()\n",
        "    train_scores  <- model(train_tokens)\n",
        "    loss <- nnf_nll_loss(train_scores, train_target)\n",
        "    cat(sprintf(\"  loss on training set: %s\", as_array(loss)), sep = \"\\n\")\n",
        "    loss$backward()\n",
        "    optimizer$step()\n",
        "  }  \n",
        "  overview[[epoch]]$loss_train <- as_array(loss)\n",
        "  ###############################################################################\n",
        "  ## Evaluation statistics of the epoch\n",
        "  ##\n",
        "  ###############################################################################\n",
        "  \n",
        "  ## Stats on training data\n",
        "  test_sentences <- traindata\n",
        "  test_tokens <- lapply(test_sentences, FUN = function(x) x$token)\n",
        "  test_scores <- model(test_tokens)\n",
        "  \n",
        "  test_target <- lapply(test_sentences, FUN = function(x) x$label_nr)\n",
        "  test_target <- unlist(test_target)\n",
        "  test_target <- torch_tensor(test_target, dtype = torch_long())\n",
        "  \n",
        "  loss <- nnf_nll_loss(test_scores, test_target)\n",
        "  overview[[epoch]]$loss_test <- as_array(loss)\n",
        "  \n",
        "  ## Stats on test data\n",
        "  test_sentences <- testdata\n",
        "  test_tokens <- lapply(test_sentences, FUN = function(x) x$token)\n",
        "  test_scores <- model(test_tokens)\n",
        "  \n",
        "  test_target <- lapply(test_sentences, FUN = function(x) x$label_nr)\n",
        "  test_target <- unlist(test_target)\n",
        "  test_target <- torch_tensor(test_target, dtype = torch_long())\n",
        "  \n",
        "  loss <- nnf_nll_loss(test_scores, test_target)\n",
        "  overview[[epoch]]$loss_test <- as_array(loss)\n",
        "  \n",
        "  stats <- list(pred = unlist(lapply(test_sentences, FUN = function(x) x$label_nr))+1,\n",
        "                obs = apply(exp(as_array(test_scores)), MARGIN=1, FUN=which.max))\n",
        "  stats$pred <- levels(conll2002$label)[stats$pred]\n",
        "  stats$obs <- levels(conll2002$label)[stats$obs]\n",
        "  stats <- crf_evaluation(pred = stats$pred, obs = stats$obs)\n",
        "  print(stats$overall)\n",
        "  overview[[epoch]]$stats <- stats\n",
        "  print(overview[[epoch]])\n",
        "  \n",
        "  ##\n",
        "  ## visualise the evolution of the loss\n",
        "  ##\n",
        "  toplot <- list(train = data.frame(type = \"train\", \n",
        "                                    epoch = sapply(overview, FUN=function(x) x$epoch),\n",
        "                                    loss = sapply(overview, FUN=function(x) x$loss_train)),\n",
        "                 test = data.frame(type = \"test\", \n",
        "                                    epoch = sapply(overview, FUN=function(x) x$epoch),\n",
        "                                    loss = sapply(overview, FUN=function(x) x$loss_test)))\n",
        "  toplot <- do.call(rbind, toplot)\n",
        "  print(lattice::xyplot(loss ~ factor(epoch), groups = factor(type), data = toplot, type = \"b\", pch = 20,\n",
        "        auto.key = list(space = \"right\", points = FALSE, lines = TRUE), xlab = \"Epoch\", ylab = \"Negative Log Loss\", main = \"Evolution of loss\"))\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-23 14:03:48 epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "ignored",
          "traceback": [
            "Error in eval(expr, envir, enclos): object 'model' not found\nTraceback:\n"
          ]
        }
      ]
    }
  ]
}